{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jainkrunal/HandGestureRecognition/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYwTJGOxMUxE",
        "colab_type": "code",
        "outputId": "21f0eddc-092c-4953-cd88-c13529d4601f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/jainkrunal/HandGestureRecognition\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HandGestureRecognition'...\n",
            "remote: Enumerating objects: 5186, done.\u001b[K\n",
            "remote: Counting objects: 100% (5186/5186), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5183/5183), done.\u001b[K\n",
            "remote: Total 5186 (delta 3), reused 5179 (delta 1), pack-reused 0\n",
            "Receiving objects: 100% (5186/5186), 69.32 MiB | 41.03 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhziSnJPMuG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage import transform\n",
        "from skimage import data\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from skimage.color import rgb2gray\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_data(data_directory):\n",
        "    directories = [d for d in os.listdir(data_directory) \n",
        "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
        "    labels = []\n",
        "    images = []\n",
        "    for d in directories:\n",
        "        label_directory = os.path.join(data_directory, d)\n",
        "        file_names = [os.path.join(label_directory, f) for f in os.listdir(label_directory)]\n",
        "        for f in file_names:\n",
        "            images.append(data.imread(f))\n",
        "            labels.append(ord(d))\n",
        "    return images, labels\n",
        "\n",
        "ROOT_PATH=\"HandGestureRecognition/Data\"\n",
        "train_data_directory=os.path.join(ROOT_PATH, \"train\")\n",
        "\n",
        "images, labels=load_data(train_data_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT_xqT_jOn-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resize images\n",
        "images32 = [transform.resize(image, (28, 28,3)) for image in images]\n",
        "images32 = np.array(images32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWUsMISHOrYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images32 = rgb2gray(np.array(images32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XM3XW-Z2ilO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(dtype = tf.float32, shape = [None, 28, 28])\n",
        "y = tf.placeholder(dtype = tf.int32, shape = [None])\n",
        "images_flat = tf.contrib.layers.flatten(x)\n",
        "logits = tf.contrib.layers.fully_connected(images_flat, 100, tf.nn.relu)\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits))\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
        "correct_pred = tf.argmax(logits, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "print(\"images_flat: \", images_flat)\n",
        "print(\"logits: \", logits)\n",
        "print(\"loss: \", loss)\n",
        "print(\"predicted_labels: \", correct_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXnRLZ1Q2y1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(201):\n",
        "        print('EPOCH', i)\n",
        "        _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x: images32, y: labels})\n",
        "        if i % 10 == 0:\n",
        "            print(\"Loss: \", loss)\n",
        "        print('DONE WITH EPOCH')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhJS28Ex3ASu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_indexes = random.sample(range(len(images32)), 10)\n",
        "sample_images = [images32[i] for i in sample_indexes]\n",
        "sample_labels = [labels[i] for i in sample_indexes]\n",
        "\n",
        "# Run the \"predicted_labels\" op.\n",
        "predicted = sess.run([correct_pred], feed_dict={x: sample_images})[0]\n",
        "                        \n",
        "# Print the real and predicted labels\n",
        "print(sample_labels)\n",
        "print(predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK3xBUZy3RKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGM_Y3azyXn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the predictions and the ground truth visually.\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for i in range(len(sample_images)):\n",
        "    truth = sample_labels[i]\n",
        "    prediction = predicted[i]\n",
        "    plt.subplot(5, 2,1+i)\n",
        "    plt.axis('off')\n",
        "    color='green' if truth == prediction else 'red'\n",
        "    plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(chr(truth), chr(prediction)), \n",
        "             fontsize=12, color=color)\n",
        "    plt.imshow(sample_images[i],cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB_8Go0d3a_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}